淘宝爬虫原型，基于Gevent
作者: roy@rootk.com
2013/5/21


%!target: html
%!options: --toc --toc-level 3
%!encoding: utf-8
%!style: /static/fancy.css

+ 项目介绍 +
基于python gevent框架的淘宝爬虫原型，支持多个节点之间负载均衡，采用类似与HTTP的协议通信。支持并发控制。

用户要求向s.taobao.com查询400个关键词的销量和宝贝总数，10秒内返回内容。

目前利用100M的带宽，可以做到平均7秒内返回，总共需要读取大约90MB的内容并经过简单正则匹配和计算。

100M的带宽可以完全利用，如果s.taobao.com不限制并发访问数量并且抓取的带宽不限，时间还可以再缩短。


+ 结构 +
++ 结构图 ++
[/static/images/crawler/crawler.png]


++ 结构说明 ++
- 整个项目建立在gevent为基础的python网络并发框架上。
- Proxy Server和Work Node之间采用简单的文本协议通信，类似与HTTP协议。
- 使用queue作为greenlet之间的通讯。
- 目前手动控制并发数量，将来会采用信号控制并发数量。


+ 功能说明 +
- Proxy Server负责接收用户的请求，跟根据一个关键词查询s.taobao.com得到最终400个关键词。
- Proxy Server将400个关键词分配到Work Node。
- Work Node接收到实际任务后，启动一定数量的greenlet并发读取s.taobao.com的内容。
- 所有Work Node将抓取到的内容返回给Porxy Server。
- Porxy Server将所有返回的内容按照json格式返回给用户，至此一个查询完成。

